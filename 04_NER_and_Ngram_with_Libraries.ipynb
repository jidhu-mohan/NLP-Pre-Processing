{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) and N-gram Language Models\n",
    "## Using Popular NLP Libraries\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Named Entity Recognition (NER)**\n",
    "   - Dictionary-based NER with spaCy's PhraseMatcher\n",
    "   - CRF-based NER with sklearn-crfsuite\n",
    "   - Pre-trained NER with spaCy\n",
    "2. **N-gram Language Models**\n",
    "   - NLTK's N-gram utilities\n",
    "   - Smoothing techniques (Laplace, Add-k, Kneser-Ney)\n",
    "   - Backoff models\n",
    "   - Perplexity evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment to install)\n",
    "# !pip install spacy nltk sklearn-crfsuite pandas numpy matplotlib seaborn\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "import nltk\n",
    "from nltk import ngrams, FreqDist\n",
    "from nltk.lm import MLE, Laplace, KneserNeyInterpolated\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('maxent_ne_chunker')\n",
    "    nltk.download('words')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Named Entity Recognition (NER)\n",
    "\n",
    "### 1.1 Dictionary-based NER using spaCy's PhraseMatcher\n",
    "\n",
    "spaCy's `PhraseMatcher` allows efficient dictionary-based entity matching with support for:\n",
    "- Fast pattern matching\n",
    "- Multi-word entities\n",
    "- Linguistic features (lemmatization, case-insensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    import os\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(f\"Loaded spaCy model: {nlp.meta['name']} v{nlp.meta['version']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictionaryNER:\n",
    "    \"\"\"Dictionary-based NER using spaCy's PhraseMatcher\"\"\"\n",
    "    \n",
    "    def __init__(self, nlp):\n",
    "        self.nlp = nlp\n",
    "        self.matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "        self.entity_patterns = {}\n",
    "    \n",
    "    def add_entities(self, entity_type, entity_list):\n",
    "        \"\"\"Add entities to the matcher\"\"\"\n",
    "        patterns = [self.nlp.make_doc(entity) for entity in entity_list]\n",
    "        self.matcher.add(entity_type, patterns)\n",
    "        self.entity_patterns[entity_type] = entity_list\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        \"\"\"Process document and add custom entities\"\"\"\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []\n",
    "        \n",
    "        for match_id, start, end in matches:\n",
    "            entity_type = self.nlp.vocab.strings[match_id]\n",
    "            span = Span(doc, start, end, label=entity_type)\n",
    "            spans.append(span)\n",
    "        \n",
    "        # Filter overlapping spans (keep longest)\n",
    "        filtered_spans = spacy.util.filter_spans(spans)\n",
    "        doc.ents = filtered_spans\n",
    "        return doc\n",
    "    \n",
    "    def visualize(self, text):\n",
    "        \"\"\"Visualize entities in text\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        doc = self(doc)\n",
    "        \n",
    "        # Display using spaCy's displacy\n",
    "        from spacy import displacy\n",
    "        displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "        \n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary-based NER\n",
    "dict_ner = DictionaryNER(nlp)\n",
    "\n",
    "# Add custom entity dictionaries\n",
    "dict_ner.add_entities(\"TECH_COMPANY\", [\n",
    "    \"Google\", \"Microsoft\", \"Apple\", \"Amazon\", \"Tesla\", \n",
    "    \"Meta\", \"Netflix\", \"OpenAI\", \"NVIDIA\", \"Intel\"\n",
    "])\n",
    "\n",
    "dict_ner.add_entities(\"TECH_PERSON\", [\n",
    "    \"Bill Gates\", \"Steve Jobs\", \"Elon Musk\", \"Jeff Bezos\",\n",
    "    \"Mark Zuckerberg\", \"Sundar Pichai\", \"Tim Cook\", \"Sam Altman\"\n",
    "])\n",
    "\n",
    "dict_ner.add_entities(\"PROGRAMMING_LANGUAGE\", [\n",
    "    \"Python\", \"Java\", \"JavaScript\", \"C++\", \"Go\", \"Rust\",\n",
    "    \"TypeScript\", \"Swift\", \"Kotlin\", \"Ruby\"\n",
    "])\n",
    "\n",
    "print(\"Dictionary-based NER initialized with custom entities:\")\n",
    "for entity_type, entities in dict_ner.entity_patterns.items():\n",
    "    print(f\"  {entity_type}: {len(entities)} entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dictionary-based NER\n",
    "test_texts = [\n",
    "    \"Bill Gates founded Microsoft and revolutionized personal computing.\",\n",
    "    \"Elon Musk leads Tesla and SpaceX, pushing boundaries in electric vehicles and space exploration.\",\n",
    "    \"Python and JavaScript are popular programming languages used at Google and Amazon.\",\n",
    "    \"Sam Altman is the CEO of OpenAI, which developed ChatGPT using advanced AI.\"\n",
    "]\n",
    "\n",
    "print(\"Dictionary-based NER Results:\\n\")\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"Example {i}: {text}\")\n",
    "    doc = nlp(text)\n",
    "    doc = dict_ner(doc)\n",
    "    \n",
    "    if doc.ents:\n",
    "        print(\"\\nEntities found:\")\n",
    "        for ent in doc.ents:\n",
    "            print(f\"  - {ent.text:25s} => {ent.label_}\")\n",
    "    else:\n",
    "        print(\"  No entities found.\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entities (works in Jupyter)\n",
    "sample_text = \"Elon Musk uses Python at Tesla and works with OpenAI on AI research.\"\n",
    "print(\"Entity Visualization:\\n\")\n",
    "doc = dict_ner.visualize(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-trained NER with spaCy\n",
    "\n",
    "spaCy comes with pre-trained NER models that can recognize standard entity types:\n",
    "- PERSON, ORG, GPE (Geopolitical Entity), LOC, DATE, TIME, MONEY, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test spaCy's pre-trained NER\n",
    "def analyze_with_spacy_ner(text):\n",
    "    \"\"\"Analyze text with spaCy's pre-trained NER\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    print(f\"Text: {text}\\n\")\n",
    "    print(\"Entities found:\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"  - {ent.text:25s} => {ent.label_:10s} ({spacy.explain(ent.label_)})\")\n",
    "    print()\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# Test examples\n",
    "examples = [\n",
    "    \"Apple Inc. was founded by Steve Jobs in Cupertino, California on April 1, 1976.\",\n",
    "    \"Google acquired YouTube for $1.65 billion in October 2006.\",\n",
    "    \"The United Nations was established in New York on October 24, 1945.\"\n",
    "]\n",
    "\n",
    "print(\"spaCy Pre-trained NER Results:\\n\")\n",
    "for text in examples:\n",
    "    analyze_with_spacy_ner(text)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 CRF-based NER with sklearn-crfsuite\n",
    "\n",
    "Conditional Random Fields (CRF) is a popular sequence labeling algorithm for NER.\n",
    "We'll use the CoNLL-2003 dataset format for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction for CRF\n",
    "def word2features(sent, i):\n",
    "    \"\"\"Extract features for word at position i\"\"\"\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    \n",
    "    # Features from previous word\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    \n",
    "    # Features from next word\n",
    "    if i < len(sent) - 1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    \n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data (BIO format)\n",
    "# Format: [(word, POS_tag, NER_tag), ...]\n",
    "\n",
    "train_sentences = [\n",
    "    [('Apple', 'NNP', 'B-ORG'), ('Inc.', 'NNP', 'I-ORG'), ('was', 'VBD', 'O'),\n",
    "     ('founded', 'VBN', 'O'), ('by', 'IN', 'O'), ('Steve', 'NNP', 'B-PER'),\n",
    "     ('Jobs', 'NNP', 'I-PER'), ('in', 'IN', 'O'), ('Cupertino', 'NNP', 'B-LOC'),\n",
    "     (',', ',', 'O'), ('California', 'NNP', 'B-LOC'), ('.', '.', 'O')],\n",
    "    \n",
    "    [('Microsoft', 'NNP', 'B-ORG'), ('Corporation', 'NNP', 'I-ORG'), ('is', 'VBZ', 'O'),\n",
    "     ('based', 'VBN', 'O'), ('in', 'IN', 'O'), ('Redmond', 'NNP', 'B-LOC'),\n",
    "     (',', ',', 'O'), ('Washington', 'NNP', 'B-LOC'), ('.', '.', 'O')],\n",
    "    \n",
    "    [('Bill', 'NNP', 'B-PER'), ('Gates', 'NNP', 'I-PER'), ('co-founded', 'VBD', 'O'),\n",
    "     ('Microsoft', 'NNP', 'B-ORG'), ('with', 'IN', 'O'), ('Paul', 'NNP', 'B-PER'),\n",
    "     ('Allen', 'NNP', 'I-PER'), ('.', '.', 'O')],\n",
    "    \n",
    "    [('Google', 'NNP', 'B-ORG'), ('was', 'VBD', 'O'), ('founded', 'VBN', 'O'),\n",
    "     ('by', 'IN', 'O'), ('Larry', 'NNP', 'B-PER'), ('Page', 'NNP', 'I-PER'),\n",
    "     ('and', 'CC', 'O'), ('Sergey', 'NNP', 'B-PER'), ('Brin', 'NNP', 'I-PER'),\n",
    "     ('in', 'IN', 'O'), ('California', 'NNP', 'B-LOC'), ('.', '.', 'O')],\n",
    "    \n",
    "    [('Tesla', 'NNP', 'B-ORG'), ('Motors', 'NNP', 'I-ORG'), ('is', 'VBZ', 'O'),\n",
    "     ('led', 'VBN', 'O'), ('by', 'IN', 'O'), ('Elon', 'NNP', 'B-PER'),\n",
    "     ('Musk', 'NNP', 'I-PER'), ('.', '.', 'O')],\n",
    "    \n",
    "    [('Amazon', 'NNP', 'B-ORG'), ('is', 'VBZ', 'O'), ('headquartered', 'VBN', 'O'),\n",
    "     ('in', 'IN', 'O'), ('Seattle', 'NNP', 'B-LOC'), (',', ',', 'O'),\n",
    "     ('Washington', 'NNP', 'B-LOC'), ('.', '.', 'O')],\n",
    "    \n",
    "    [('Mark', 'NNP', 'B-PER'), ('Zuckerberg', 'NNP', 'I-PER'), ('founded', 'VBD', 'O'),\n",
    "     ('Facebook', 'NNP', 'B-ORG'), ('in', 'IN', 'O'), ('2004', 'CD', 'B-DATE'), ('.', '.', 'O')],\n",
    "    \n",
    "    [('Tim', 'NNP', 'B-PER'), ('Cook', 'NNP', 'I-PER'), ('is', 'VBZ', 'O'),\n",
    "     ('the', 'DT', 'O'), ('CEO', 'NN', 'O'), ('of', 'IN', 'O'),\n",
    "     ('Apple', 'NNP', 'B-ORG'), ('.', '.', 'O')],\n",
    "]\n",
    "\n",
    "test_sentences = [\n",
    "    [('Jeff', 'NNP', 'B-PER'), ('Bezos', 'NNP', 'I-PER'), ('founded', 'VBD', 'O'),\n",
    "     ('Amazon', 'NNP', 'B-ORG'), ('in', 'IN', 'O'), ('1994', 'CD', 'B-DATE'), ('.', '.', 'O')],\n",
    "    \n",
    "    [('Sundar', 'NNP', 'B-PER'), ('Pichai', 'NNP', 'I-PER'), ('leads', 'VBZ', 'O'),\n",
    "     ('Google', 'NNP', 'B-ORG'), ('and', 'CC', 'O'), ('Alphabet', 'NNP', 'B-ORG'), ('.', '.', 'O')],\n",
    "    \n",
    "    [('Netflix', 'NNP', 'B-ORG'), ('operates', 'VBZ', 'O'), ('from', 'IN', 'O'),\n",
    "     ('Los', 'NNP', 'B-LOC'), ('Gatos', 'NNP', 'I-LOC'), (',', ',', 'O'),\n",
    "     ('California', 'NNP', 'B-LOC'), ('.', '.', 'O')],\n",
    "]\n",
    "\n",
    "print(f\"Training data: {len(train_sentences)} sentences\")\n",
    "print(f\"Test data: {len(test_sentences)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for CRF\n",
    "X_train = [sent2features(s) for s in train_sentences]\n",
    "y_train = [sent2labels(s) for s in train_sentences]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sentences]\n",
    "y_test = [sent2labels(s) for s in test_sentences]\n",
    "\n",
    "# Train CRF model\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,          # L1 regularization coefficient\n",
    "    c2=0.1,          # L2 regularization coefficient\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Training CRF model...\\n\")\n",
    "crf.fit(X_train, y_train)\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "# Display results\n",
    "print(\"CRF-based NER Predictions:\\n\")\n",
    "for i, sent in enumerate(test_sentences):\n",
    "    print(f\"Sentence {i+1}:\")\n",
    "    tokens = sent2tokens(sent)\n",
    "    print(\" \".join(tokens))\n",
    "    print()\n",
    "    print(f\"{'Token':<20} {'True Label':<15} {'Predicted':<15} {'Match'}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for j, token in enumerate(tokens):\n",
    "        true_label = y_test[i][j]\n",
    "        pred_label = y_pred[i][j]\n",
    "        match = \"✓\" if true_label == pred_label else \"✗\"\n",
    "        print(f\"{token:<20} {true_label:<15} {pred_label:<15} {match}\")\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CRF model\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')  # Remove 'O' label for entity-focused evaluation\n",
    "\n",
    "print(\"CRF Model Performance Report:\\n\")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect most important features\n",
    "def print_transitions(trans_features, top_n=10):\n",
    "    \"\"\"Print top transition features\"\"\"\n",
    "    print(f\"\\nTop {top_n} Transition Features:\")\n",
    "    for (label_from, label_to), weight in trans_features[:top_n]:\n",
    "        print(f\"  {label_from:10s} -> {label_to:10s}  {weight:>8.3f}\")\n",
    "\n",
    "def print_state_features(state_features, top_n=20):\n",
    "    \"\"\"Print top state features\"\"\"\n",
    "    print(f\"\\nTop {top_n} State Features:\")\n",
    "    for (attr, label), weight in state_features[:top_n]:\n",
    "        print(f\"  {attr:40s} => {label:10s}  {weight:>8.3f}\")\n",
    "\n",
    "# Get feature importance\n",
    "print(\"Feature Importance Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Transition features\n",
    "trans_features = Counter(crf.transition_features_).most_common(10)\n",
    "print_transitions(trans_features)\n",
    "\n",
    "# State features\n",
    "state_features = Counter(crf.state_features_).most_common(20)\n",
    "print_state_features(state_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: N-gram Language Models with NLTK\n",
    "\n",
    "NLTK provides robust tools for building and evaluating n-gram language models with various smoothing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Building N-gram Models with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus for language modeling\n",
    "train_corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"the cat sat on the log\",\n",
    "    \"the dog sat on the mat\",\n",
    "    \"a cat is a pet\",\n",
    "    \"a dog is a pet\",\n",
    "    \"the cat likes the mat\",\n",
    "    \"the dog likes the log\",\n",
    "    \"cats and dogs are pets\",\n",
    "    \"the mat is soft\",\n",
    "    \"the log is hard\",\n",
    "    \"pets are nice\",\n",
    "    \"the cat sleeps on the mat\",\n",
    "    \"the dog runs to the log\",\n",
    "]\n",
    "\n",
    "test_corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"a cat is a good pet\",\n",
    "    \"the mat is very soft\",\n",
    "]\n",
    "\n",
    "# Tokenize corpus\n",
    "train_tokenized = [sent.split() for sent in train_corpus]\n",
    "test_tokenized = [sent.split() for sent in test_corpus]\n",
    "\n",
    "print(f\"Training corpus: {len(train_tokenized)} sentences\")\n",
    "print(f\"Test corpus: {len(test_tokenized)} sentences\")\n",
    "print(f\"\\nSample sentences:\")\n",
    "for sent in train_tokenized[:3]:\n",
    "    print(f\"  {' '.join(sent)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary and n-grams\n",
    "n = 3  # trigram model\n",
    "\n",
    "# Pad sentences and create n-grams\n",
    "train_data, vocab = padded_everygram_pipeline(n, train_tokenized)\n",
    "\n",
    "# Separate train data for each model (need to regenerate as it's a generator)\n",
    "train_data_mle, vocab_mle = padded_everygram_pipeline(n, train_tokenized)\n",
    "train_data_laplace, vocab_laplace = padded_everygram_pipeline(n, train_tokenized)\n",
    "train_data_kn, vocab_kn = padded_everygram_pipeline(n, train_tokenized)\n",
    "\n",
    "print(f\"Building {n}-gram models...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Smoothing Techniques\n",
    "\n",
    "#### 2.2.1 Maximum Likelihood Estimation (MLE) - No Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE model (no smoothing)\n",
    "lm_mle = MLE(n)\n",
    "lm_mle.fit(train_data_mle, vocab_mle)\n",
    "\n",
    "print(\"MLE Model (No Smoothing)\")\n",
    "print(f\"Vocabulary size: {len(lm_mle.vocab)}\")\n",
    "print(f\"N-gram order: {lm_mle.order}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Laplace Smoothing (Add-One)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laplace smoothing model\n",
    "lm_laplace = Laplace(n)\n",
    "lm_laplace.fit(train_data_laplace, vocab_laplace)\n",
    "\n",
    "print(\"Laplace Smoothing Model\")\n",
    "print(f\"Vocabulary size: {len(lm_laplace.vocab)}\")\n",
    "print(f\"N-gram order: {lm_laplace.order}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Kneser-Ney Smoothing (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kneser-Ney smoothing model\n",
    "lm_kn = KneserNeyInterpolated(n)\n",
    "lm_kn.fit(train_data_kn, vocab_kn)\n",
    "\n",
    "print(\"Kneser-Ney Interpolated Smoothing Model\")\n",
    "print(f\"Vocabulary size: {len(lm_kn.vocab)}\")\n",
    "print(f\"N-gram order: {lm_kn.order}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Comparing Probabilities with Different Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test various n-grams\n",
    "test_ngrams = [\n",
    "    ('cat',),                    # Unigram (seen)\n",
    "    ('elephant',),               # Unigram (unseen)\n",
    "    ('the', 'cat'),              # Bigram (seen)\n",
    "    ('the', 'elephant'),         # Bigram (unseen)\n",
    "    ('cat', 'sat', 'on'),        # Trigram (seen)\n",
    "    ('cat', 'jumped', 'over'),   # Trigram (unseen)\n",
    "]\n",
    "\n",
    "# Compare probabilities\n",
    "results = []\n",
    "for ngram in test_ngrams:\n",
    "    # Get context (all but last word)\n",
    "    context = ngram[:-1] if len(ngram) > 1 else ()\n",
    "    word = ngram[-1]\n",
    "    \n",
    "    # Calculate probabilities with different models\n",
    "    prob_mle = lm_mle.score(word, context)\n",
    "    prob_laplace = lm_laplace.score(word, context)\n",
    "    prob_kn = lm_kn.score(word, context)\n",
    "    \n",
    "    results.append({\n",
    "        'N-gram': ' '.join(ngram),\n",
    "        'Order': len(ngram),\n",
    "        'MLE': prob_mle,\n",
    "        'Laplace': prob_laplace,\n",
    "        'Kneser-Ney': prob_kn\n",
    "    })\n",
    "\n",
    "# Display as DataFrame\n",
    "df_probs = pd.DataFrame(results)\n",
    "print(\"\\nProbability Comparison Across Smoothing Methods:\\n\")\n",
    "print(df_probs.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "x = np.arange(len(df_probs))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, df_probs['MLE'], width, label='MLE', alpha=0.8)\n",
    "ax.bar(x, df_probs['Laplace'], width, label='Laplace', alpha=0.8)\n",
    "ax.bar(x + width, df_probs['Kneser-Ney'], width, label='Kneser-Ney', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('N-grams', fontsize=12)\n",
    "ax.set_ylabel('Probability', fontsize=12)\n",
    "ax.set_title('N-gram Probabilities with Different Smoothing Methods', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_probs['N-gram'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Backoff Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_backoff(lm, ngram):\n",
    "    \"\"\"Show how model backs off to lower-order n-grams\"\"\"\n",
    "    print(f\"\\nN-gram: {' '.join(ngram)}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Try different orders\n",
    "    for order in range(len(ngram), 0, -1):\n",
    "        current_ngram = ngram[-order:]\n",
    "        context = current_ngram[:-1] if len(current_ngram) > 1 else ()\n",
    "        word = current_ngram[-1]\n",
    "        \n",
    "        prob = lm.score(word, context)\n",
    "        print(f\"  {order}-gram: P({word}|{' '.join(context) if context else 'ε'}) = {prob:.6f}\")\n",
    "\n",
    "# Demonstrate backoff with Kneser-Ney model\n",
    "print(\"Backoff Behavior with Kneser-Ney Smoothing:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "backoff_examples = [\n",
    "    ('the', 'cat', 'sat'),\n",
    "    ('the', 'cat', 'jumped'),\n",
    "    ('the', 'elephant', 'danced'),\n",
    "]\n",
    "\n",
    "for example in backoff_examples:\n",
    "    demonstrate_backoff(lm_kn, example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Perplexity Evaluation\n",
    "\n",
    "Perplexity measures how well a language model predicts a test corpus.\n",
    "- Lower perplexity = better model\n",
    "- Perplexity of K means the model is as confused as choosing uniformly from K words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "test_data = [list(pad_both_ends(sent, n=n)) for sent in test_tokenized]\n",
    "\n",
    "# Calculate perplexity for each model\n",
    "perplexities = []\n",
    "\n",
    "for model_name, model in [(\"MLE\", lm_mle), (\"Laplace\", lm_laplace), (\"Kneser-Ney\", lm_kn)]:\n",
    "    perplexity = model.perplexity(test_data)\n",
    "    perplexities.append({\n",
    "        'Model': model_name,\n",
    "        'Perplexity': perplexity\n",
    "    })\n",
    "    print(f\"{model_name:15s} Perplexity: {perplexity:.4f}\")\n",
    "\n",
    "# Visualize perplexity\n",
    "df_perplexity = pd.DataFrame(perplexities)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(df_perplexity['Model'], df_perplexity['Perplexity'], \n",
    "               color=['#ff9999', '#66b3ff', '#99ff99'], alpha=0.8)\n",
    "plt.xlabel('Smoothing Method', fontsize=12)\n",
    "plt.ylabel('Perplexity', fontsize=12)\n",
    "plt.title('Model Perplexity Comparison (Lower is Better)', fontsize=14)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Text Generation with N-gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lm, num_words=10, text_seed=None, random_seed=42):\n",
    "    \"\"\"Generate text using n-gram language model\"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Start with padding or seed\n",
    "    if text_seed:\n",
    "        content = text_seed.split()\n",
    "    else:\n",
    "        content = ['<s>'] * (lm.order - 1)\n",
    "    \n",
    "    for _ in range(num_words):\n",
    "        # Get context\n",
    "        context = tuple(content[-(lm.order - 1):])\n",
    "        \n",
    "        # Generate next word\n",
    "        next_word = lm.generate(1, text_seed=context)\n",
    "        \n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "        \n",
    "        content.append(next_word)\n",
    "    \n",
    "    # Remove padding tokens\n",
    "    generated = [w for w in content if w not in ['<s>', '</s>']]\n",
    "    return ' '.join(generated)\n",
    "\n",
    "# Generate text with different models\n",
    "print(\"Generated Text Samples:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name, model in [(\"Laplace\", lm_laplace), (\"Kneser-Ney\", lm_kn)]:\n",
    "    print(f\"\\n{model_name} Model:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i in range(5):\n",
    "        text = generate_text(model, num_words=8, random_seed=42+i)\n",
    "        print(f\"  {i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with seed\n",
    "print(\"\\nText Generation with Seed:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "seeds = [\"the cat\", \"the dog\", \"a pet\"]\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\nSeed: '{seed}'\")\n",
    "    text = generate_text(lm_kn, num_words=6, text_seed=seed)\n",
    "    print(f\"Generated: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Entropy and Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(lm, test_data):\n",
    "    \"\"\"Calculate cross-entropy of language model on test data\"\"\"\n",
    "    return lm.entropy(test_data)\n",
    "\n",
    "print(\"Cross-Entropy Analysis:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name, model in [(\"MLE\", lm_mle), (\"Laplace\", lm_laplace), (\"Kneser-Ney\", lm_kn)]:\n",
    "    try:\n",
    "        entropy = calculate_entropy(model, test_data)\n",
    "        perplexity = 2 ** entropy\n",
    "        print(f\"{model_name:15s} - Entropy: {entropy:.4f}, Perplexity: {perplexity:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{model_name:15s} - Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 N-gram Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze n-gram frequencies\n",
    "from nltk import bigrams, trigrams\n",
    "\n",
    "# Flatten training corpus\n",
    "all_words = [word for sent in train_tokenized for word in sent]\n",
    "\n",
    "# Get bigrams and trigrams\n",
    "bigram_list = list(bigrams(all_words))\n",
    "trigram_list = list(trigrams(all_words))\n",
    "\n",
    "# Count frequencies\n",
    "bigram_freq = FreqDist(bigram_list)\n",
    "trigram_freq = FreqDist(trigram_list)\n",
    "\n",
    "print(\"Top 10 Bigrams:\")\n",
    "print(\"-\" * 40)\n",
    "for (w1, w2), count in bigram_freq.most_common(10):\n",
    "    print(f\"  ({w1}, {w2}): {count}\")\n",
    "\n",
    "print(\"\\nTop 10 Trigrams:\")\n",
    "print(\"-\" * 40)\n",
    "for (w1, w2, w3), count in trigram_freq.most_common(10):\n",
    "    print(f\"  ({w1}, {w2}, {w3}): {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bigram frequencies\n",
    "top_bigrams = bigram_freq.most_common(15)\n",
    "bigram_labels = [f\"{w1} {w2}\" for (w1, w2), _ in top_bigrams]\n",
    "bigram_counts = [count for _, count in top_bigrams]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(bigram_labels, bigram_counts, color='steelblue', alpha=0.8)\n",
    "plt.xlabel('Frequency', fontsize=12)\n",
    "plt.ylabel('Bigrams', fontsize=12)\n",
    "plt.title('Top 15 Most Frequent Bigrams', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### Named Entity Recognition (NER)\n",
    "\n",
    "1. **Dictionary-based NER**\n",
    "   - ✅ Fast and accurate for known entities\n",
    "   - ✅ Easy to implement with spaCy's PhraseMatcher\n",
    "   - ❌ Cannot handle unseen entities\n",
    "   - ❌ Requires manual dictionary maintenance\n",
    "\n",
    "2. **CRF-based NER**\n",
    "   - ✅ Learns from training data\n",
    "   - ✅ Can generalize to unseen entities\n",
    "   - ✅ Considers context and features\n",
    "   - ❌ Requires labeled training data\n",
    "   - ❌ More computationally intensive\n",
    "\n",
    "3. **Pre-trained NER (spaCy)**\n",
    "   - ✅ Ready to use out-of-the-box\n",
    "   - ✅ Good performance on standard entities\n",
    "   - ✅ Supports multiple languages\n",
    "\n",
    "### N-gram Language Models\n",
    "\n",
    "1. **Smoothing Techniques**\n",
    "   - **MLE**: No smoothing, zero probability for unseen n-grams\n",
    "   - **Laplace**: Simple add-one smoothing, tends to over-smooth\n",
    "   - **Kneser-Ney**: Advanced smoothing, best performance\n",
    "\n",
    "2. **Backoff**\n",
    "   - Falls back to lower-order n-grams when data is sparse\n",
    "   - Improves robustness and generalization\n",
    "\n",
    "3. **Perplexity**\n",
    "   - Primary evaluation metric for language models\n",
    "   - Lower perplexity indicates better model\n",
    "   - Related to cross-entropy: Perplexity = 2^(cross-entropy)\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Use **dictionary-based NER** for domain-specific, well-defined entities\n",
    "- Use **CRF-based NER** when you have training data and need generalization\n",
    "- Use **pre-trained models** as a baseline before building custom models\n",
    "- For language models, **Kneser-Ney smoothing** generally performs best\n",
    "- Higher-order n-grams (trigrams, 4-grams) capture more context but require more data\n",
    "- Always evaluate on held-out test data\n",
    "\n",
    "### Libraries Used\n",
    "\n",
    "- **spaCy**: Industrial-strength NLP with pre-trained models\n",
    "- **NLTK**: Comprehensive NLP toolkit with language modeling utilities\n",
    "- **sklearn-crfsuite**: Efficient CRF implementation for sequence labeling\n",
    "- **pandas/matplotlib/seaborn**: Data analysis and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "1. **NER Exercise**: Create a custom entity recognizer for your domain (e.g., medical terms, product names)\n",
    "\n",
    "2. **CRF Features**: Experiment with different feature sets (word shapes, character n-grams, etc.)\n",
    "\n",
    "3. **Language Model**: Train n-gram models on a larger corpus (e.g., news articles, Wikipedia)\n",
    "\n",
    "4. **Smoothing Comparison**: Compare different smoothing techniques on various corpus sizes\n",
    "\n",
    "5. **Text Generation**: Build an autocomplete system using n-gram models\n",
    "\n",
    "6. **Evaluation**: Implement additional metrics (F1-score for NER, BLEU score for generation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
