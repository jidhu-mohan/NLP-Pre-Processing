{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) and N-gram Language Models\n",
    "\n",
    "This notebook covers:\n",
    "1. **Named Entity Recognition (NER)**\n",
    "   - Dictionary-based methods\n",
    "   - CRF-based methods\n",
    "2. **N-gram Language Models**\n",
    "   - Smoothing techniques\n",
    "   - Backoff methods\n",
    "   - Perplexity evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install sklearn-crfsuite nltk spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Named Entity Recognition (NER)\n",
    "\n",
    "Named Entity Recognition is the task of identifying and classifying named entities in text into predefined categories such as:\n",
    "- **PERSON**: Names of people\n",
    "- **ORGANIZATION**: Companies, agencies, institutions\n",
    "- **LOCATION**: Countries, cities, states\n",
    "- **DATE**: Absolute or relative dates\n",
    "- **MONEY**: Monetary values\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dictionary-based NER\n",
    "\n",
    "Dictionary-based NER uses predefined lists of entities to identify and classify named entities in text.\n",
    "\n",
    "**Advantages:**\n",
    "- Simple to implement\n",
    "- Fast and efficient\n",
    "- High precision for known entities\n",
    "\n",
    "**Disadvantages:**\n",
    "- Limited to known entities\n",
    "- Cannot handle variations or new entities\n",
    "- Requires maintenance of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictionaryBasedNER:\n",
    "    \"\"\"Simple dictionary-based Named Entity Recognition\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize entity dictionaries\n",
    "        self.entity_dicts = {\n",
    "            'PERSON': {\n",
    "                'John Smith', 'Mary Johnson', 'Robert Brown', 'Emma Watson',\n",
    "                'Bill Gates', 'Steve Jobs', 'Elon Musk', 'Jeff Bezos'\n",
    "            },\n",
    "            'ORGANIZATION': {\n",
    "                'Google', 'Microsoft', 'Apple', 'Amazon', 'Tesla',\n",
    "                'Stanford University', 'MIT', 'NASA', 'FBI', 'United Nations'\n",
    "            },\n",
    "            'LOCATION': {\n",
    "                'New York', 'London', 'Paris', 'Tokyo', 'California',\n",
    "                'United States', 'England', 'France', 'Japan', 'India'\n",
    "            },\n",
    "            'DATE': {\n",
    "                'Monday', 'Tuesday', 'January', 'February', 'today',\n",
    "                'yesterday', 'tomorrow', 'next week'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Create a combined lookup dictionary for faster matching\n",
    "        self.entity_lookup = {}\n",
    "        for entity_type, entities in self.entity_dicts.items():\n",
    "            for entity in entities:\n",
    "                self.entity_lookup[entity.lower()] = entity_type\n",
    "    \n",
    "    def add_entity(self, entity_type, entity_name):\n",
    "        \"\"\"Add a new entity to the dictionary\"\"\"\n",
    "        if entity_type not in self.entity_dicts:\n",
    "            self.entity_dicts[entity_type] = set()\n",
    "        self.entity_dicts[entity_type].add(entity_name)\n",
    "        self.entity_lookup[entity_name.lower()] = entity_type\n",
    "    \n",
    "    def recognize(self, text):\n",
    "        \"\"\"Recognize entities in text using dictionary matching\"\"\"\n",
    "        entities = []\n",
    "        words = text.split()\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            # Try matching multi-word entities (up to 3 words)\n",
    "            for n in range(min(3, len(words) - i), 0, -1):\n",
    "                phrase = ' '.join(words[i:i+n])\n",
    "                phrase_clean = re.sub(r'[^\\w\\s]', '', phrase)  # Remove punctuation\n",
    "                \n",
    "                if phrase_clean.lower() in self.entity_lookup:\n",
    "                    entity_type = self.entity_lookup[phrase_clean.lower()]\n",
    "                    entities.append({\n",
    "                        'text': phrase_clean,\n",
    "                        'type': entity_type,\n",
    "                        'start': i,\n",
    "                        'end': i + n\n",
    "                    })\n",
    "                    i += n\n",
    "                    break\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def display_entities(self, text):\n",
    "        \"\"\"Display recognized entities with their types\"\"\"\n",
    "        entities = self.recognize(text)\n",
    "        print(f\"Text: {text}\\n\")\n",
    "        print(\"Recognized Entities:\")\n",
    "        for entity in entities:\n",
    "            print(f\"  - {entity['text']:20s} => {entity['type']}\")\n",
    "        return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of Dictionary-based NER\n",
    "dict_ner = DictionaryBasedNER()\n",
    "\n",
    "sample_texts = [\n",
    "    \"Bill Gates founded Microsoft in the United States.\",\n",
    "    \"Elon Musk runs Tesla and lives in California.\",\n",
    "    \"Emma Watson studied at Stanford University in California.\"\n",
    "]\n",
    "\n",
    "for text in sample_texts:\n",
    "    dict_ner.display_entities(text)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 CRF-based NER\n",
    "\n",
    "Conditional Random Fields (CRF) is a probabilistic model for sequence labeling that considers:\n",
    "- **Context**: Surrounding words and their features\n",
    "- **Sequential dependencies**: Relationships between adjacent labels\n",
    "- **Features**: Word shape, capitalization, POS tags, etc.\n",
    "\n",
    "**Advantages:**\n",
    "- Can learn from training data\n",
    "- Handles unknown entities\n",
    "- Considers context and sequence\n",
    "\n",
    "**Disadvantages:**\n",
    "- Requires labeled training data\n",
    "- More complex than dictionary-based\n",
    "- Slower inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction for CRF\n",
    "def word2features(sent, i):\n",
    "    \"\"\"Extract features for a word at position i in sentence\"\"\"\n",
    "    word = sent[i][0]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    \n",
    "    # Features for previous word\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True  # Beginning of sentence\n",
    "    \n",
    "    # Features for next word\n",
    "    if i < len(sent) - 1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True  # End of sentence\n",
    "    \n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    \"\"\"Extract features for all words in a sentence\"\"\"\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    \"\"\"Extract labels for all words in a sentence\"\"\"\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    \"\"\"Extract tokens from a sentence\"\"\"\n",
    "    return [token for token, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training data in BIO format\n",
    "# B-X: Beginning of entity type X\n",
    "# I-X: Inside (continuation) of entity type X\n",
    "# O: Outside (not an entity)\n",
    "\n",
    "train_sentences = [\n",
    "    [('Bill', 'B-PERSON'), ('Gates', 'I-PERSON'), ('founded', 'O'), \n",
    "     ('Microsoft', 'B-ORG'), ('in', 'O'), ('1975', 'B-DATE')],\n",
    "    \n",
    "    [('Steve', 'B-PERSON'), ('Jobs', 'I-PERSON'), ('co-founded', 'O'), \n",
    "     ('Apple', 'B-ORG'), ('in', 'O'), ('California', 'B-LOC')],\n",
    "    \n",
    "    [('Elon', 'B-PERSON'), ('Musk', 'I-PERSON'), ('leads', 'O'), \n",
    "     ('Tesla', 'B-ORG'), ('and', 'O'), ('SpaceX', 'B-ORG')],\n",
    "    \n",
    "    [('Google', 'B-ORG'), ('was', 'O'), ('founded', 'O'), ('in', 'O'), \n",
    "     ('California', 'B-LOC'), ('by', 'O'), ('Larry', 'B-PERSON'), ('Page', 'I-PERSON')],\n",
    "    \n",
    "    [('Amazon', 'B-ORG'), ('is', 'O'), ('based', 'O'), ('in', 'O'), \n",
    "     ('Seattle', 'B-LOC'), (',', 'O'), ('Washington', 'B-LOC')],\n",
    "]\n",
    "\n",
    "test_sentences = [\n",
    "    [('Jeff', 'B-PERSON'), ('Bezos', 'I-PERSON'), ('founded', 'O'), \n",
    "     ('Amazon', 'B-ORG'), ('in', 'O'), ('1994', 'B-DATE')],\n",
    "    \n",
    "    [('Mark', 'B-PERSON'), ('Zuckerberg', 'I-PERSON'), ('created', 'O'), \n",
    "     ('Facebook', 'B-ORG'), ('at', 'O'), ('Harvard', 'B-ORG')],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CRF model\n",
    "try:\n",
    "    import sklearn_crfsuite\n",
    "    from sklearn_crfsuite import metrics\n",
    "    \n",
    "    # Prepare training data\n",
    "    X_train = [sent2features(s) for s in train_sentences]\n",
    "    y_train = [sent2labels(s) for s in train_sentences]\n",
    "    \n",
    "    X_test = [sent2features(s) for s in test_sentences]\n",
    "    y_test = [sent2labels(s) for s in test_sentences]\n",
    "    \n",
    "    # Train CRF model\n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "        algorithm='lbfgs',\n",
    "        c1=0.1,\n",
    "        c2=0.1,\n",
    "        max_iterations=100,\n",
    "        all_possible_transitions=True\n",
    "    )\n",
    "    \n",
    "    crf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = crf.predict(X_test)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"CRF-based NER Results:\\n\")\n",
    "    for i, sent in enumerate(test_sentences):\n",
    "        print(f\"Sentence {i+1}:\")\n",
    "        for j, (token, true_label) in enumerate(sent):\n",
    "            pred_label = y_pred[i][j]\n",
    "            match = \"✓\" if true_label == pred_label else \"✗\"\n",
    "            print(f\"  {token:15s} True: {true_label:10s} Pred: {pred_label:10s} {match}\")\n",
    "        print()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    labels = list(crf.classes_)\n",
    "    labels.remove('O')  # Remove 'O' label for evaluation\n",
    "    \n",
    "    print(\"\\nModel Performance:\")\n",
    "    print(metrics.flat_classification_report(\n",
    "        y_test, y_pred, labels=labels, digits=3\n",
    "    ))\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"sklearn-crfsuite not installed. Run: pip install sklearn-crfsuite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Comparison: Dictionary-based vs CRF-based NER\n",
    "\n",
    "| Aspect | Dictionary-based | CRF-based |\n",
    "|--------|-----------------|----------|\n",
    "| **Training Data** | Not required | Required |\n",
    "| **New Entities** | Cannot detect | Can detect |\n",
    "| **Context Awareness** | No | Yes |\n",
    "| **Speed** | Very fast | Slower |\n",
    "| **Precision** | High (for known) | Moderate to High |\n",
    "| **Recall** | Low | Moderate to High |\n",
    "| **Maintenance** | Manual updates | Automatic learning |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: N-gram Language Models\n",
    "\n",
    "N-gram language models predict the probability of a word given the previous (n-1) words.\n",
    "\n",
    "**Types:**\n",
    "- **Unigram (1-gram)**: P(w) - probability of a word\n",
    "- **Bigram (2-gram)**: P(w|w₁) - probability given previous word\n",
    "- **Trigram (3-gram)**: P(w|w₁,w₂) - probability given previous 2 words\n",
    "\n",
    "**Formula (Bigram):**\n",
    "```\n",
    "P(w₂|w₁) = Count(w₁, w₂) / Count(w₁)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramLanguageModel:\n",
    "    \"\"\"N-gram language model with smoothing and backoff\"\"\"\n",
    "    \n",
    "    def __init__(self, n=2):\n",
    "        self.n = n\n",
    "        self.ngram_counts = defaultdict(int)\n",
    "        self.context_counts = defaultdict(int)\n",
    "        self.vocabulary = set()\n",
    "        self.total_words = 0\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Simple tokenization\"\"\"\n",
    "        # Add sentence boundaries\n",
    "        tokens = ['<s>'] * (self.n - 1) + text.lower().split() + ['</s>']\n",
    "        return tokens\n",
    "    \n",
    "    def train(self, corpus):\n",
    "        \"\"\"Train the n-gram model on a corpus\"\"\"\n",
    "        for text in corpus:\n",
    "            tokens = self.tokenize(text)\n",
    "            self.vocabulary.update(tokens)\n",
    "            self.total_words += len(tokens)\n",
    "            \n",
    "            # Count n-grams\n",
    "            for i in range(len(tokens) - self.n + 1):\n",
    "                ngram = tuple(tokens[i:i + self.n])\n",
    "                context = ngram[:-1]\n",
    "                \n",
    "                self.ngram_counts[ngram] += 1\n",
    "                self.context_counts[context] += 1\n",
    "    \n",
    "    def probability(self, ngram, smoothing='none'):\n",
    "        \"\"\"Calculate probability of an n-gram with optional smoothing\"\"\"\n",
    "        if isinstance(ngram, str):\n",
    "            ngram = tuple(ngram.lower().split())\n",
    "        \n",
    "        if smoothing == 'none':\n",
    "            return self._prob_no_smoothing(ngram)\n",
    "        elif smoothing == 'laplace':\n",
    "            return self._prob_laplace(ngram)\n",
    "        elif smoothing == 'add_k':\n",
    "            return self._prob_add_k(ngram, k=0.5)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown smoothing method: {smoothing}\")\n",
    "    \n",
    "    def _prob_no_smoothing(self, ngram):\n",
    "        \"\"\"Probability without smoothing (MLE)\"\"\"\n",
    "        context = ngram[:-1]\n",
    "        context_count = self.context_counts[context]\n",
    "        \n",
    "        if context_count == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return self.ngram_counts[ngram] / context_count\n",
    "    \n",
    "    def _prob_laplace(self, ngram):\n",
    "        \"\"\"Laplace (add-one) smoothing\"\"\"\n",
    "        context = ngram[:-1]\n",
    "        V = len(self.vocabulary)\n",
    "        \n",
    "        numerator = self.ngram_counts[ngram] + 1\n",
    "        denominator = self.context_counts[context] + V\n",
    "        \n",
    "        return numerator / denominator\n",
    "    \n",
    "    def _prob_add_k(self, ngram, k=0.5):\n",
    "        \"\"\"Add-k smoothing\"\"\"\n",
    "        context = ngram[:-1]\n",
    "        V = len(self.vocabulary)\n",
    "        \n",
    "        numerator = self.ngram_counts[ngram] + k\n",
    "        denominator = self.context_counts[context] + k * V\n",
    "        \n",
    "        return numerator / denominator\n",
    "    \n",
    "    def perplexity(self, test_corpus, smoothing='laplace'):\n",
    "        \"\"\"Calculate perplexity on test corpus\"\"\"\n",
    "        total_log_prob = 0\n",
    "        total_words = 0\n",
    "        \n",
    "        for text in test_corpus:\n",
    "            tokens = self.tokenize(text)\n",
    "            \n",
    "            for i in range(len(tokens) - self.n + 1):\n",
    "                ngram = tuple(tokens[i:i + self.n])\n",
    "                prob = self.probability(ngram, smoothing=smoothing)\n",
    "                \n",
    "                if prob > 0:\n",
    "                    total_log_prob += np.log2(prob)\n",
    "                else:\n",
    "                    # Assign small probability for unseen n-grams\n",
    "                    total_log_prob += np.log2(1e-10)\n",
    "                \n",
    "                total_words += 1\n",
    "        \n",
    "        # Perplexity = 2^(-log_prob / N)\n",
    "        avg_log_prob = total_log_prob / total_words\n",
    "        perplexity = 2 ** (-avg_log_prob)\n",
    "        \n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Training an N-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training corpus\n",
    "train_corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"the cat sat on the log\",\n",
    "    \"the dog sat on the mat\",\n",
    "    \"a cat is a pet\",\n",
    "    \"a dog is a pet\",\n",
    "    \"the cat likes the mat\",\n",
    "    \"the dog likes the log\",\n",
    "]\n",
    "\n",
    "# Train bigram model\n",
    "bigram_model = NgramLanguageModel(n=2)\n",
    "bigram_model.train(train_corpus)\n",
    "\n",
    "print(f\"Vocabulary size: {len(bigram_model.vocabulary)}\")\n",
    "print(f\"Total words: {bigram_model.total_words}\")\n",
    "print(f\"Number of unique bigrams: {len(bigram_model.ngram_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Smoothing Techniques\n",
    "\n",
    "Smoothing addresses the **zero probability problem** where unseen n-grams get probability 0.\n",
    "\n",
    "#### Types of Smoothing:\n",
    "\n",
    "1. **Laplace (Add-One) Smoothing**\n",
    "   - Add 1 to all counts\n",
    "   - Formula: P(w|context) = (Count + 1) / (Context_Count + V)\n",
    "\n",
    "2. **Add-k Smoothing**\n",
    "   - Add k (0 < k < 1) to all counts\n",
    "   - Formula: P(w|context) = (Count + k) / (Context_Count + k×V)\n",
    "\n",
    "3. **Good-Turing Smoothing**\n",
    "   - Use frequency of frequencies\n",
    "   - Redistributes probability mass from seen to unseen n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare probabilities with different smoothing methods\n",
    "test_bigrams = [\n",
    "    ('the', 'cat'),    # Seen bigram\n",
    "    ('the', 'dog'),    # Seen bigram\n",
    "    ('the', 'bird'),   # Unseen bigram\n",
    "    ('cat', 'sat'),    # Seen bigram\n",
    "]\n",
    "\n",
    "print(\"Bigram Probabilities with Different Smoothing:\\n\")\n",
    "print(f\"{'Bigram':<20} {'No Smoothing':<15} {'Laplace':<15} {'Add-k (k=0.5)':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for bigram in test_bigrams:\n",
    "    prob_none = bigram_model.probability(bigram, smoothing='none')\n",
    "    prob_laplace = bigram_model.probability(bigram, smoothing='laplace')\n",
    "    prob_add_k = bigram_model.probability(bigram, smoothing='add_k')\n",
    "    \n",
    "    bigram_str = f\"{bigram[0]} {bigram[1]}\"\n",
    "    print(f\"{bigram_str:<20} {prob_none:<15.6f} {prob_laplace:<15.6f} {prob_add_k:<15.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Backoff Methods\n",
    "\n",
    "Backoff uses lower-order n-grams when higher-order n-grams are not available.\n",
    "\n",
    "**Katz Backoff:**\n",
    "```\n",
    "P(wₙ|w₁...wₙ₋₁) = {\n",
    "    P*(wₙ|w₁...wₙ₋₁)           if count(w₁...wₙ) > 0\n",
    "    α(w₁...wₙ₋₁) × P(wₙ|w₂...wₙ₋₁)   otherwise\n",
    "}\n",
    "```\n",
    "\n",
    "where α is a backoff weight to ensure probabilities sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackoffLanguageModel:\n",
    "    \"\"\"N-gram language model with backoff\"\"\"\n",
    "    \n",
    "    def __init__(self, max_n=3):\n",
    "        self.max_n = max_n\n",
    "        # Train models of different orders\n",
    "        self.models = {n: NgramLanguageModel(n=n) for n in range(1, max_n + 1)}\n",
    "    \n",
    "    def train(self, corpus):\n",
    "        \"\"\"Train all n-gram models\"\"\"\n",
    "        for n, model in self.models.items():\n",
    "            model.train(corpus)\n",
    "    \n",
    "    def probability_with_backoff(self, ngram, smoothing='laplace'):\n",
    "        \"\"\"Calculate probability using backoff\"\"\"\n",
    "        if isinstance(ngram, str):\n",
    "            ngram = tuple(ngram.lower().split())\n",
    "        \n",
    "        n = len(ngram)\n",
    "        \n",
    "        # Try from highest order to lowest\n",
    "        for order in range(min(n, self.max_n), 0, -1):\n",
    "            current_ngram = ngram[-order:]\n",
    "            model = self.models[order]\n",
    "            \n",
    "            prob = model.probability(current_ngram, smoothing=smoothing)\n",
    "            \n",
    "            if prob > 0 or order == 1:\n",
    "                return prob, order\n",
    "        \n",
    "        return 0.0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train backoff model\n",
    "backoff_model = BackoffLanguageModel(max_n=3)\n",
    "backoff_model.train(train_corpus)\n",
    "\n",
    "# Test backoff behavior\n",
    "test_trigrams = [\n",
    "    ('the', 'cat', 'sat'),     # Seen trigram\n",
    "    ('the', 'cat', 'jumped'),  # Unseen trigram, backs off to bigram\n",
    "    ('the', 'bird', 'flew'),   # Unseen, backs off further\n",
    "]\n",
    "\n",
    "print(\"Backoff Behavior:\\n\")\n",
    "for trigram in test_trigrams:\n",
    "    prob, order_used = backoff_model.probability_with_backoff(trigram)\n",
    "    trigram_str = ' '.join(trigram)\n",
    "    print(f\"Trigram: {trigram_str:<25} Prob: {prob:.6f}  (used {order}-gram)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Perplexity\n",
    "\n",
    "Perplexity measures how well a language model predicts a test corpus.\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "Perplexity = 2^(-1/N × Σ log₂ P(wᵢ|context))\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "- Lower perplexity = better model\n",
    "- Perplexity of K means the model is as confused as if it had to choose uniformly among K possibilities\n",
    "- Typical values: 50-1000 for natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test corpus (similar but different from training)\n",
    "test_corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"a cat is a good pet\",\n",
    "]\n",
    "\n",
    "# Calculate perplexity with different smoothing\n",
    "print(\"Perplexity Comparison:\\n\")\n",
    "\n",
    "for smoothing in ['laplace', 'add_k']:\n",
    "    perplexity = bigram_model.perplexity(test_corpus, smoothing=smoothing)\n",
    "    print(f\"{smoothing.capitalize():15s} smoothing: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different n-gram orders\n",
    "print(\"\\nPerplexity by N-gram Order (with Laplace smoothing):\\n\")\n",
    "\n",
    "for n in range(1, 4):\n",
    "    model = NgramLanguageModel(n=n)\n",
    "    model.train(train_corpus)\n",
    "    perplexity = model.perplexity(test_corpus, smoothing='laplace')\n",
    "    print(f\"{n}-gram model: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Practical Example: Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_token='<s>', max_length=10, smoothing='laplace'):\n",
    "    \"\"\"Generate text using n-gram model\"\"\"\n",
    "    generated = [start_token] * (model.n - 1)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Get context\n",
    "        context = tuple(generated[-(model.n - 1):])\n",
    "        \n",
    "        # Calculate probabilities for all possible next words\n",
    "        candidates = {}\n",
    "        for word in model.vocabulary:\n",
    "            if word not in ['<s>', '</s>']:\n",
    "                ngram = context + (word,)\n",
    "                prob = model.probability(ngram, smoothing=smoothing)\n",
    "                candidates[word] = prob\n",
    "        \n",
    "        # Normalize probabilities\n",
    "        total = sum(candidates.values())\n",
    "        if total > 0:\n",
    "            candidates = {w: p/total for w, p in candidates.items()}\n",
    "        \n",
    "        # Sample next word\n",
    "        if candidates:\n",
    "            words = list(candidates.keys())\n",
    "            probs = list(candidates.values())\n",
    "            next_word = np.random.choice(words, p=probs)\n",
    "            generated.append(next_word)\n",
    "            \n",
    "            if next_word == '</s>':\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Remove start tokens and return\n",
    "    return ' '.join([w for w in generated if w not in ['<s>', '</s>']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some text samples\n",
    "print(\"Generated Text Samples (Bigram Model):\\n\")\n",
    "for i in range(5):\n",
    "    text = generate_text(bigram_model, max_length=8)\n",
    "    print(f\"{i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Named Entity Recognition (NER)\n",
    "\n",
    "1. **Dictionary-based NER**\n",
    "   - Simple and fast\n",
    "   - Requires manual curation\n",
    "   - Best for known entities\n",
    "\n",
    "2. **CRF-based NER**\n",
    "   - Machine learning approach\n",
    "   - Learns from context\n",
    "   - Better generalization\n",
    "\n",
    "### N-gram Language Models\n",
    "\n",
    "1. **Smoothing**\n",
    "   - Laplace (Add-one): Simple but over-smooths\n",
    "   - Add-k: More flexible\n",
    "   - Good-Turing: Better probability distribution\n",
    "\n",
    "2. **Backoff**\n",
    "   - Uses lower-order n-grams when data is sparse\n",
    "   - Improves robustness\n",
    "\n",
    "3. **Perplexity**\n",
    "   - Evaluation metric\n",
    "   - Lower is better\n",
    "   - Measures model uncertainty\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- NER is crucial for information extraction\n",
    "- Dictionary-based methods are simple but limited\n",
    "- CRF-based methods provide better generalization\n",
    "- N-gram models are fundamental for language modeling\n",
    "- Smoothing is essential to handle unseen n-grams\n",
    "- Backoff improves robustness with sparse data\n",
    "- Perplexity quantifies model quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "1. **NER Exercise**: Expand the dictionary-based NER with more entity types (DATE, MONEY, etc.)\n",
    "\n",
    "2. **CRF Exercise**: Add more features to the CRF model (word prefixes, word length, etc.)\n",
    "\n",
    "3. **N-gram Exercise**: Implement a trigram model and compare with bigram\n",
    "\n",
    "4. **Smoothing Exercise**: Implement Good-Turing smoothing\n",
    "\n",
    "5. **Application Exercise**: Build a text autocomplete system using n-grams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
