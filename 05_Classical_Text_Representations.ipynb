{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Text Representations: Bag-of-Words (BoW) and TF-IDF\n",
    "\n",
    "This notebook covers classical text representation techniques that transform text into numerical vectors for machine learning.\n",
    "\n",
    "## Topics Covered:\n",
    "1. **Bag-of-Words (BoW)**\n",
    "   - Basic concepts and intuition\n",
    "   - Implementation from scratch\n",
    "   - Using sklearn's CountVectorizer\n",
    "   - Binary BoW and N-gram BoW\n",
    "   \n",
    "2. **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
    "   - Understanding TF, IDF, and TF-IDF\n",
    "   - Implementation from scratch\n",
    "   - Using sklearn's TfidfVectorizer\n",
    "   - Variants and normalization\n",
    "   \n",
    "3. **Practical Applications**\n",
    "   - Text classification\n",
    "   - Document similarity\n",
    "   - Information retrieval\n",
    "   - Feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install scikit-learn nltk pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# NLTK imports\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Bag-of-Words (BoW)\n",
    "\n",
    "### What is Bag-of-Words?\n",
    "\n",
    "Bag-of-Words is a text representation technique that:\n",
    "- Treats text as an unordered collection of words\n",
    "- Ignores grammar and word order\n",
    "- Represents documents as vectors of word counts\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Documents:\n",
    "  D1: \"I love machine learning\"\n",
    "  D2: \"I love deep learning\"\n",
    "  \n",
    "Vocabulary: [I, love, machine, learning, deep]\n",
    "\n",
    "BoW Vectors:\n",
    "  D1: [1, 1, 1, 1, 0]\n",
    "  D2: [1, 1, 0, 1, 1]\n",
    "```\n",
    "\n",
    "### Advantages:\n",
    "- ✅ Simple and easy to understand\n",
    "- ✅ Works well for many NLP tasks\n",
    "- ✅ Fast to compute\n",
    "\n",
    "### Disadvantages:\n",
    "- ❌ Ignores word order and context\n",
    "- ❌ High-dimensional and sparse\n",
    "- ❌ Doesn't capture semantic meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Implementing Bag-of-Words from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBagOfWords:\n",
    "    \"\"\"Simple Bag-of-Words implementation from scratch\"\"\"\n",
    "    \n",
    "    def __init__(self, lowercase=True, remove_punctuation=True):\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.vocabulary = None\n",
    "        self.word_to_idx = None\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Preprocess text\"\"\"\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        if self.remove_punctuation:\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        return text.split()\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"Build vocabulary from documents\"\"\"\n",
    "        vocab_set = set()\n",
    "        for doc in documents:\n",
    "            words = self.preprocess(doc)\n",
    "            vocab_set.update(words)\n",
    "        \n",
    "        self.vocabulary = sorted(list(vocab_set))\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocabulary)}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        \"\"\"Transform documents to BoW vectors\"\"\"\n",
    "        if self.vocabulary is None:\n",
    "            raise ValueError(\"Vocabulary not built. Call fit() first.\")\n",
    "        \n",
    "        vectors = []\n",
    "        for doc in documents:\n",
    "            words = self.preprocess(doc)\n",
    "            vector = [0] * len(self.vocabulary)\n",
    "            \n",
    "            for word in words:\n",
    "                if word in self.word_to_idx:\n",
    "                    idx = self.word_to_idx[word]\n",
    "                    vector[idx] += 1\n",
    "            \n",
    "            vectors.append(vector)\n",
    "        \n",
    "        return np.array(vectors)\n",
    "    \n",
    "    def fit_transform(self, documents):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        return self.fit(documents).transform(documents)\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Get vocabulary words\"\"\"\n",
    "        return self.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "sample_docs = [\n",
    "    \"I love machine learning\",\n",
    "    \"I love deep learning\",\n",
    "    \"Machine learning is awesome\",\n",
    "    \"Deep learning uses neural networks\",\n",
    "]\n",
    "\n",
    "# Create and fit BoW model\n",
    "bow = SimpleBagOfWords()\n",
    "bow_vectors = bow.fit_transform(sample_docs)\n",
    "\n",
    "# Display results\n",
    "print(\"Bag-of-Words Representation\\n\")\n",
    "print(f\"Vocabulary size: {len(bow.vocabulary)}\")\n",
    "print(f\"Vocabulary: {bow.vocabulary}\\n\")\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "bow_df = pd.DataFrame(bow_vectors, \n",
    "                      columns=bow.get_feature_names(),\n",
    "                      index=[f\"Doc {i+1}\" for i in range(len(sample_docs))])\n",
    "\n",
    "print(\"BoW Matrix:\")\n",
    "print(bow_df)\n",
    "\n",
    "# Show original documents\n",
    "print(\"\\nOriginal Documents:\")\n",
    "for i, doc in enumerate(sample_docs, 1):\n",
    "    print(f\"  Doc {i}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize BoW matrix\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(bow_df, annot=True, fmt='d', cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'Word Count'}, linewidths=0.5)\n",
    "plt.title('Bag-of-Words Matrix Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Words (Vocabulary)', fontsize=12)\n",
    "plt.ylabel('Documents', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Using sklearn's CountVectorizer\n",
    "\n",
    "sklearn provides a more feature-rich implementation with:\n",
    "- Built-in preprocessing\n",
    "- N-gram support\n",
    "- Stop word filtering\n",
    "- Min/max document frequency filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CountVectorizer\n",
    "count_vectorizer = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',  # Remove common English stop words\n",
    "    max_features=20,       # Keep only top 20 features\n",
    ")\n",
    "\n",
    "# Larger corpus for demonstration\n",
    "corpus = [\n",
    "    \"Machine learning is a subset of artificial intelligence\",\n",
    "    \"Deep learning is a subset of machine learning\",\n",
    "    \"Neural networks are the foundation of deep learning\",\n",
    "    \"Natural language processing is an important AI application\",\n",
    "    \"Computer vision is another important AI application\",\n",
    "    \"Reinforcement learning helps agents learn from environment\",\n",
    "    \"Supervised learning requires labeled training data\",\n",
    "    \"Unsupervised learning finds patterns in unlabeled data\",\n",
    "]\n",
    "\n",
    "# Fit and transform\n",
    "bow_matrix = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert to DataFrame\n",
    "bow_df_sklearn = pd.DataFrame(\n",
    "    bow_matrix.toarray(),\n",
    "    columns=count_vectorizer.get_feature_names_out(),\n",
    "    index=[f\"Doc {i+1}\" for i in range(len(corpus))]\n",
    ")\n",
    "\n",
    "print(\"sklearn CountVectorizer Results\\n\")\n",
    "print(f\"Vocabulary size: {len(count_vectorizer.vocabulary_)}\")\n",
    "print(f\"Shape: {bow_matrix.shape}\\n\")\n",
    "print(\"BoW Matrix:\")\n",
    "print(bow_df_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word frequencies across all documents\n",
    "word_counts = bow_df_sklearn.sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "word_counts.plot(kind='bar', color='steelblue', alpha=0.8)\n",
    "plt.title('Word Frequency Across All Documents', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Words', fontsize=12)\n",
    "plt.ylabel('Total Count', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Binary Bag-of-Words\n",
    "\n",
    "Binary BoW only tracks presence/absence of words (0 or 1), not counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary BoW\n",
    "binary_vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "binary_bow = binary_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compare regular vs binary\n",
    "comparison_docs = [\n",
    "    \"learning learning learning machine machine\",\n",
    "    \"learning machine\"\n",
    "]\n",
    "\n",
    "regular_vec = count_vectorizer.transform(comparison_docs)\n",
    "binary_vec = binary_vectorizer.transform(comparison_docs)\n",
    "\n",
    "print(\"Regular BoW vs Binary BoW\\n\")\n",
    "print(\"Documents:\")\n",
    "for i, doc in enumerate(comparison_docs, 1):\n",
    "    print(f\"  Doc {i}: {doc}\")\n",
    "\n",
    "print(\"\\nRegular BoW (counts):\")\n",
    "print(pd.DataFrame(regular_vec.toarray(), \n",
    "                   columns=count_vectorizer.get_feature_names_out()))\n",
    "\n",
    "print(\"\\nBinary BoW (presence/absence):\")\n",
    "print(pd.DataFrame(binary_vec.toarray(), \n",
    "                   columns=binary_vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 N-gram Bag-of-Words\n",
    "\n",
    "N-grams capture sequences of words:\n",
    "- **Unigram**: Single words (\"machine\", \"learning\")\n",
    "- **Bigram**: Two consecutive words (\"machine learning\")\n",
    "- **Trigram**: Three consecutive words (\"deep machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram examples\n",
    "ngram_docs = [\n",
    "    \"natural language processing\",\n",
    "    \"machine learning algorithms\",\n",
    "    \"deep learning neural networks\",\n",
    "]\n",
    "\n",
    "# Unigram (1,1)\n",
    "unigram_vec = CountVectorizer(ngram_range=(1, 1))\n",
    "unigram_bow = unigram_vec.fit_transform(ngram_docs)\n",
    "\n",
    "# Bigram (2,2)\n",
    "bigram_vec = CountVectorizer(ngram_range=(2, 2))\n",
    "bigram_bow = bigram_vec.fit_transform(ngram_docs)\n",
    "\n",
    "# Unigram + Bigram (1,2)\n",
    "combined_vec = CountVectorizer(ngram_range=(1, 2))\n",
    "combined_bow = combined_vec.fit_transform(ngram_docs)\n",
    "\n",
    "print(\"N-gram Bag-of-Words Examples\\n\")\n",
    "print(\"Documents:\")\n",
    "for i, doc in enumerate(ngram_docs, 1):\n",
    "    print(f\"  {i}. {doc}\")\n",
    "\n",
    "print(f\"\\nUnigrams (1,1): {unigram_vec.get_feature_names_out().tolist()}\")\n",
    "print(f\"Bigrams (2,2): {bigram_vec.get_feature_names_out().tolist()}\")\n",
    "print(f\"Combined (1,2): {combined_vec.get_feature_names_out().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize combined n-grams\n",
    "combined_df = pd.DataFrame(\n",
    "    combined_bow.toarray(),\n",
    "    columns=combined_vec.get_feature_names_out(),\n",
    "    index=[f\"Doc {i+1}\" for i in range(len(ngram_docs))]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "sns.heatmap(combined_df, annot=True, fmt='d', cmap='Blues', \n",
    "            cbar_kws={'label': 'Count'}, linewidths=1)\n",
    "plt.title('Unigram + Bigram BoW Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Documents', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "### What is TF-IDF?\n",
    "\n",
    "TF-IDF is a statistical measure that evaluates the importance of a word in a document relative to a corpus.\n",
    "\n",
    "### Components:\n",
    "\n",
    "1. **Term Frequency (TF)**: How often a word appears in a document\n",
    "   ```\n",
    "   TF(t, d) = (Count of term t in document d) / (Total terms in document d)\n",
    "   ```\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**: How rare/common a word is across documents\n",
    "   ```\n",
    "   IDF(t) = log(Total documents / Documents containing term t)\n",
    "   ```\n",
    "\n",
    "3. **TF-IDF**: Combination of both\n",
    "   ```\n",
    "   TF-IDF(t, d) = TF(t, d) × IDF(t)\n",
    "   ```\n",
    "\n",
    "### Intuition:\n",
    "- ✅ High TF-IDF: Word appears frequently in a document but rarely in others → **Important**\n",
    "- ❌ Low TF-IDF: Word is common across all documents → **Less important** (e.g., \"the\", \"is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implementing TF-IDF from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTFIDF:\n",
    "    \"\"\"Simple TF-IDF implementation from scratch\"\"\"\n",
    "    \n",
    "    def __init__(self, lowercase=True, remove_punctuation=True):\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.vocabulary = None\n",
    "        self.word_to_idx = None\n",
    "        self.idf_values = None\n",
    "        self.n_docs = 0\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Preprocess text\"\"\"\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        if self.remove_punctuation:\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        return text.split()\n",
    "    \n",
    "    def compute_tf(self, words):\n",
    "        \"\"\"Compute term frequency\"\"\"\n",
    "        tf = {}\n",
    "        word_count = len(words)\n",
    "        word_freq = Counter(words)\n",
    "        \n",
    "        for word, count in word_freq.items():\n",
    "            tf[word] = count / word_count\n",
    "        \n",
    "        return tf\n",
    "    \n",
    "    def compute_idf(self, documents):\n",
    "        \"\"\"Compute inverse document frequency\"\"\"\n",
    "        self.n_docs = len(documents)\n",
    "        doc_freq = Counter()\n",
    "        \n",
    "        # Count document frequency for each word\n",
    "        for doc in documents:\n",
    "            words = set(self.preprocess(doc))\n",
    "            doc_freq.update(words)\n",
    "        \n",
    "        # Compute IDF\n",
    "        idf = {}\n",
    "        for word, freq in doc_freq.items():\n",
    "            idf[word] = np.log((self.n_docs + 1) / (freq + 1)) + 1  # Smoothed IDF\n",
    "        \n",
    "        return idf\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"Build vocabulary and compute IDF\"\"\"\n",
    "        vocab_set = set()\n",
    "        for doc in documents:\n",
    "            words = self.preprocess(doc)\n",
    "            vocab_set.update(words)\n",
    "        \n",
    "        self.vocabulary = sorted(list(vocab_set))\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocabulary)}\n",
    "        self.idf_values = self.compute_idf(documents)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        \"\"\"Transform documents to TF-IDF vectors\"\"\"\n",
    "        if self.vocabulary is None:\n",
    "            raise ValueError(\"Vocabulary not built. Call fit() first.\")\n",
    "        \n",
    "        tfidf_vectors = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            words = self.preprocess(doc)\n",
    "            tf = self.compute_tf(words)\n",
    "            \n",
    "            # Create TF-IDF vector\n",
    "            vector = [0.0] * len(self.vocabulary)\n",
    "            for word, tf_value in tf.items():\n",
    "                if word in self.word_to_idx:\n",
    "                    idx = self.word_to_idx[word]\n",
    "                    idf_value = self.idf_values.get(word, 0)\n",
    "                    vector[idx] = tf_value * idf_value\n",
    "            \n",
    "            tfidf_vectors.append(vector)\n",
    "        \n",
    "        return np.array(tfidf_vectors)\n",
    "    \n",
    "    def fit_transform(self, documents):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        return self.fit(documents).transform(documents)\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Get vocabulary words\"\"\"\n",
    "        return self.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents for TF-IDF\n",
    "tfidf_docs = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog sat on the log\",\n",
    "    \"Cats and dogs are great pets\",\n",
    "    \"Python is a great programming language\",\n",
    "]\n",
    "\n",
    "# Create and fit TF-IDF model\n",
    "tfidf = SimpleTFIDF()\n",
    "tfidf_vectors = tfidf.fit_transform(tfidf_docs)\n",
    "\n",
    "# Create DataFrame\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_vectors,\n",
    "    columns=tfidf.get_feature_names(),\n",
    "    index=[f\"Doc {i+1}\" for i in range(len(tfidf_docs))]\n",
    ")\n",
    "\n",
    "print(\"TF-IDF from Scratch\\n\")\n",
    "print(f\"Vocabulary size: {len(tfidf.vocabulary)}\\n\")\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_df.round(3))\n",
    "\n",
    "print(\"\\nOriginal Documents:\")\n",
    "for i, doc in enumerate(tfidf_docs, 1):\n",
    "    print(f\"  Doc {i}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize IDF values\n",
    "idf_series = pd.Series(tfidf.idf_values).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "idf_series.plot(kind='bar', color='coral', alpha=0.8)\n",
    "plt.title('IDF Values for Each Word', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Words', fontsize=12)\n",
    "plt.ylabel('IDF Value', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Higher IDF = Rare word (more informative)\")\n",
    "print(\"  - Lower IDF = Common word (less informative)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Step-by-Step TF-IDF Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed TF-IDF calculation for one document\n",
    "example_doc = \"The cat sat on the mat\"\n",
    "print(f\"Document: '{example_doc}'\\n\")\n",
    "\n",
    "words = tfidf.preprocess(example_doc)\n",
    "print(f\"Preprocessed words: {words}\\n\")\n",
    "\n",
    "# Compute TF\n",
    "tf_dict = tfidf.compute_tf(words)\n",
    "print(\"Term Frequency (TF):\")\n",
    "for word, tf_val in sorted(tf_dict.items()):\n",
    "    print(f\"  {word:12s}: {tf_val:.3f}  (appears {words.count(word)} times out of {len(words)} words)\")\n",
    "\n",
    "# Show IDF\n",
    "print(\"\\nInverse Document Frequency (IDF):\")\n",
    "for word in sorted(tf_dict.keys()):\n",
    "    idf_val = tfidf.idf_values.get(word, 0)\n",
    "    doc_count = sum(1 for doc in tfidf_docs if word in tfidf.preprocess(doc))\n",
    "    print(f\"  {word:12s}: {idf_val:.3f}  (appears in {doc_count}/{len(tfidf_docs)} documents)\")\n",
    "\n",
    "# Compute TF-IDF\n",
    "print(\"\\nTF-IDF (TF × IDF):\")\n",
    "for word in sorted(tf_dict.keys()):\n",
    "    tf_val = tf_dict[word]\n",
    "    idf_val = tfidf.idf_values.get(word, 0)\n",
    "    tfidf_val = tf_val * idf_val\n",
    "    print(f\"  {word:12s}: {tf_val:.3f} × {idf_val:.3f} = {tfidf_val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Using sklearn's TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    max_features=20,\n",
    "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "    norm='l2'            # L2 normalization\n",
    ")\n",
    "\n",
    "# Use the larger corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert to DataFrame\n",
    "tfidf_df_sklearn = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=tfidf_vectorizer.get_feature_names_out(),\n",
    "    index=[f\"Doc {i+1}\" for i in range(len(corpus))]\n",
    ")\n",
    "\n",
    "print(\"sklearn TfidfVectorizer Results\\n\")\n",
    "print(f\"Shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\\n\")\n",
    "print(\"TF-IDF Matrix (rounded to 3 decimals):\")\n",
    "print(tfidf_df_sklearn.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TF-IDF matrix\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(tfidf_df_sklearn, annot=True, fmt='.2f', cmap='RdYlGn', \n",
    "            cbar_kws={'label': 'TF-IDF Score'}, linewidths=0.5)\n",
    "plt.title('TF-IDF Matrix Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Documents', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Comparing BoW and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare BoW vs TF-IDF on same document\n",
    "comparison_corpus = [\n",
    "    \"machine learning is great\",\n",
    "    \"machine learning machine learning machine learning\",  # Repeated words\n",
    "]\n",
    "\n",
    "# BoW\n",
    "bow_comp = CountVectorizer()\n",
    "bow_result = bow_comp.fit_transform(comparison_corpus)\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_comp = TfidfVectorizer()\n",
    "tfidf_result = tfidf_comp.fit_transform(comparison_corpus)\n",
    "\n",
    "# Display comparison\n",
    "print(\"Comparison: BoW vs TF-IDF\\n\")\n",
    "print(\"Documents:\")\n",
    "for i, doc in enumerate(comparison_corpus, 1):\n",
    "    print(f\"  Doc {i}: {doc}\")\n",
    "\n",
    "print(\"\\nBag-of-Words (Raw Counts):\")\n",
    "bow_comp_df = pd.DataFrame(bow_result.toarray(), \n",
    "                           columns=bow_comp.get_feature_names_out(),\n",
    "                           index=[\"Doc 1\", \"Doc 2\"])\n",
    "print(bow_comp_df)\n",
    "\n",
    "print(\"\\nTF-IDF (Normalized Importance):\")\n",
    "tfidf_comp_df = pd.DataFrame(tfidf_result.toarray(), \n",
    "                             columns=tfidf_comp.get_feature_names_out(),\n",
    "                             index=[\"Doc 1\", \"Doc 2\"])\n",
    "print(tfidf_comp_df.round(3))\n",
    "\n",
    "print(\"\\nKey Observation:\")\n",
    "print(\"  - BoW: Doc 2 has much higher counts for 'machine' and 'learning'\")\n",
    "print(\"  - TF-IDF: Values are normalized, reducing the impact of repetition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Practical Applications\n",
    "\n",
    "### 3.1 Document Similarity with Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents for similarity comparison\n",
    "docs_similarity = [\n",
    "    \"Machine learning is a branch of artificial intelligence\",\n",
    "    \"Deep learning is a subset of machine learning\",\n",
    "    \"Natural language processing deals with text and speech\",\n",
    "    \"Computer vision enables machines to understand images\",\n",
    "    \"AI and machine learning are transforming industries\",\n",
    "]\n",
    "\n",
    "# Compute TF-IDF\n",
    "tfidf_sim = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix_sim = tfidf_sim.fit_transform(docs_similarity)\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix_sim)\n",
    "\n",
    "# Create DataFrame\n",
    "similarity_df = pd.DataFrame(\n",
    "    cosine_sim,\n",
    "    index=[f\"Doc {i+1}\" for i in range(len(docs_similarity))],\n",
    "    columns=[f\"Doc {i+1}\" for i in range(len(docs_similarity))]\n",
    ")\n",
    "\n",
    "print(\"Document Similarity Matrix (Cosine Similarity)\\n\")\n",
    "print(similarity_df.round(3))\n",
    "\n",
    "print(\"\\nDocuments:\")\n",
    "for i, doc in enumerate(docs_similarity, 1):\n",
    "    print(f\"  Doc {i}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarity_df, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "            vmin=0, vmax=1, square=True, linewidths=1,\n",
    "            cbar_kws={'label': 'Cosine Similarity'})\n",
    "plt.title('Document Similarity Matrix\\n(1.0 = Identical, 0.0 = Completely Different)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find most similar pairs\n",
    "print(\"\\nMost Similar Document Pairs:\\n\")\n",
    "similarity_pairs = []\n",
    "for i in range(len(cosine_sim)):\n",
    "    for j in range(i+1, len(cosine_sim)):\n",
    "        similarity_pairs.append((i+1, j+1, cosine_sim[i][j]))\n",
    "\n",
    "similarity_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "for rank, (doc1, doc2, score) in enumerate(similarity_pairs[:3], 1):\n",
    "    print(f\"{rank}. Doc {doc1} ↔ Doc {doc2}: {score:.3f}\")\n",
    "    print(f\"   Doc {doc1}: {docs_similarity[doc1-1]}\")\n",
    "    print(f\"   Doc {doc2}: {docs_similarity[doc2-1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Text Classification with BoW and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset for classification\n",
    "texts = [\n",
    "    # Technology\n",
    "    \"Machine learning algorithms can predict outcomes\",\n",
    "    \"Artificial intelligence is transforming industries\",\n",
    "    \"Deep learning uses neural networks for pattern recognition\",\n",
    "    \"Python is a popular programming language for data science\",\n",
    "    \"Cloud computing provides scalable infrastructure\",\n",
    "    \"Blockchain technology enables secure transactions\",\n",
    "    \"Big data analytics helps make informed decisions\",\n",
    "    \"IoT devices connect everyday objects to the internet\",\n",
    "    \n",
    "    # Sports\n",
    "    \"The football team won the championship game\",\n",
    "    \"Basketball players train hard for the tournament\",\n",
    "    \"The tennis match was very exciting and competitive\",\n",
    "    \"Olympic athletes compete for gold medals\",\n",
    "    \"The soccer team scored three goals in the match\",\n",
    "    \"Baseball season starts in spring every year\",\n",
    "    \"Swimmers broke multiple world records at the event\",\n",
    "    \"The cricket match lasted for five days\",\n",
    "    \n",
    "    # Health\n",
    "    \"Regular exercise improves cardiovascular health\",\n",
    "    \"A balanced diet is essential for good nutrition\",\n",
    "    \"Meditation reduces stress and anxiety levels\",\n",
    "    \"Vaccines help prevent infectious diseases\",\n",
    "    \"Adequate sleep is crucial for mental health\",\n",
    "    \"Yoga improves flexibility and strength\",\n",
    "    \"Healthy lifestyle choices reduce disease risk\",\n",
    "    \"Regular checkups help detect health issues early\",\n",
    "]\n",
    "\n",
    "labels = ['tech'] * 8 + ['sports'] * 8 + ['health'] * 8\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Categories: {set(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare BoW vs TF-IDF for classification\n",
    "def train_and_evaluate(vectorizer, vectorizer_name):\n",
    "    \"\"\"Train and evaluate classifier\"\"\"\n",
    "    # Transform data\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Train classifier\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(X_train_vec, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = classifier.predict(X_test_vec)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{vectorizer_name} + Naive Bayes Classifier\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\\n\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return y_pred, accuracy\n",
    "\n",
    "# Test with BoW\n",
    "bow_vectorizer = CountVectorizer(stop_words='english', max_features=50)\n",
    "y_pred_bow, acc_bow = train_and_evaluate(bow_vectorizer, \"Bag-of-Words\")\n",
    "\n",
    "# Test with TF-IDF\n",
    "tfidf_vectorizer_clf = TfidfVectorizer(stop_words='english', max_features=50)\n",
    "y_pred_tfidf, acc_tfidf = train_and_evaluate(tfidf_vectorizer_clf, \"TF-IDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# BoW confusion matrix\n",
    "cm_bow = confusion_matrix(y_test, y_pred_bow, labels=['tech', 'sports', 'health'])\n",
    "sns.heatmap(cm_bow, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['tech', 'sports', 'health'],\n",
    "            yticklabels=['tech', 'sports', 'health'])\n",
    "axes[0].set_title(f'Bag-of-Words\\nAccuracy: {acc_bow:.3f}', fontweight='bold')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# TF-IDF confusion matrix\n",
    "cm_tfidf = confusion_matrix(y_test, y_pred_tfidf, labels=['tech', 'sports', 'health'])\n",
    "sns.heatmap(cm_tfidf, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['tech', 'sports', 'health'],\n",
    "            yticklabels=['tech', 'sports', 'health'])\n",
    "axes[1].set_title(f'TF-IDF\\nAccuracy: {acc_tfidf:.3f}', fontweight='bold')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier with TF-IDF to analyze feature importance\n",
    "tfidf_feat = TfidfVectorizer(stop_words='english', max_features=30)\n",
    "X_train_feat = tfidf_feat.fit_transform(X_train)\n",
    "\n",
    "classifier_feat = MultinomialNB()\n",
    "classifier_feat.fit(X_train_feat, y_train)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf_feat.get_feature_names_out()\n",
    "\n",
    "# Get feature importance for each class\n",
    "feature_importance = {}\n",
    "for i, category in enumerate(classifier_feat.classes_):\n",
    "    # Get log probabilities for this class\n",
    "    log_probs = classifier_feat.feature_log_prob_[i]\n",
    "    \n",
    "    # Sort by importance\n",
    "    top_indices = np.argsort(log_probs)[-10:][::-1]\n",
    "    top_features = [(feature_names[idx], log_probs[idx]) for idx in top_indices]\n",
    "    \n",
    "    feature_importance[category] = top_features\n",
    "\n",
    "# Display feature importance\n",
    "print(\"Top 10 Most Important Features for Each Category:\\n\")\n",
    "for category, features in feature_importance.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    print(\"-\" * 40)\n",
    "    for rank, (feature, score) in enumerate(features, 1):\n",
    "        print(f\"  {rank:2d}. {feature:20s} (score: {score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for idx, (category, features) in enumerate(feature_importance.items()):\n",
    "    words = [f[0] for f in features]\n",
    "    scores = [f[1] for f in features]\n",
    "    \n",
    "    axes[idx].barh(range(len(words)), scores, color='skyblue', alpha=0.8)\n",
    "    axes[idx].set_yticks(range(len(words)))\n",
    "    axes[idx].set_yticklabels(words)\n",
    "    axes[idx].set_xlabel('Log Probability', fontsize=10)\n",
    "    axes[idx].set_title(f'{category.upper()}\\nTop Features', fontweight='bold', fontsize=12)\n",
    "    axes[idx].invert_yaxis()\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Information Retrieval: Simple Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSearchEngine:\n",
    "    \"\"\"Simple search engine using TF-IDF\"\"\"\n",
    "    \n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        self.doc_vectors = self.vectorizer.fit_transform(documents)\n",
    "    \n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"Search for most relevant documents\"\"\"\n",
    "        # Transform query\n",
    "        query_vector = self.vectorizer.transform([query])\n",
    "        \n",
    "        # Compute similarity\n",
    "        similarities = cosine_similarity(query_vector, self.doc_vectors)[0]\n",
    "        \n",
    "        # Get top k results\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] > 0:\n",
    "                results.append({\n",
    "                    'rank': len(results) + 1,\n",
    "                    'doc_id': idx,\n",
    "                    'document': self.documents[idx],\n",
    "                    'similarity': similarities[idx]\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def display_results(self, query, results):\n",
    "        \"\"\"Display search results\"\"\"\n",
    "        print(f\"Query: '{query}'\\n\")\n",
    "        print(f\"Found {len(results)} relevant documents:\\n\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for result in results:\n",
    "            print(f\"\\nRank {result['rank']} (Similarity: {result['similarity']:.4f})\")\n",
    "            print(f\"Doc {result['doc_id']}: {result['document']}\")\n",
    "            print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create search engine with all documents\n",
    "search_engine = SimpleSearchEngine(texts)\n",
    "\n",
    "# Test queries\n",
    "queries = [\n",
    "    \"machine learning and artificial intelligence\",\n",
    "    \"sports competition and athletes\",\n",
    "    \"healthy lifestyle and exercise\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    results = search_engine.search(query, top_k=3)\n",
    "    search_engine.display_results(query, results)\n",
    "    print(\"\\n\" + \"#\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Best Practices\n",
    "\n",
    "### Bag-of-Words (BoW)\n",
    "\n",
    "**When to use:**\n",
    "- ✅ Simple baseline for text classification\n",
    "- ✅ When word frequency is important\n",
    "- ✅ Limited computational resources\n",
    "\n",
    "**Limitations:**\n",
    "- ❌ Ignores word order and context\n",
    "- ❌ Doesn't capture semantic relationships\n",
    "- ❌ High dimensionality (sparse vectors)\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "**When to use:**\n",
    "- ✅ Information retrieval and search\n",
    "- ✅ Document similarity tasks\n",
    "- ✅ When rare words are more important\n",
    "- ✅ Generally better than BoW for most tasks\n",
    "\n",
    "**Advantages over BoW:**\n",
    "- ✅ Reduces impact of common words\n",
    "- ✅ Highlights distinctive terms\n",
    "- ✅ Better for ranking and retrieval\n",
    "\n",
    "### Key Comparisons\n",
    "\n",
    "| Aspect | Bag-of-Words | TF-IDF |\n",
    "|--------|--------------|--------|\n",
    "| **Representation** | Raw counts | Weighted importance |\n",
    "| **Common words** | High values | Downweighted |\n",
    "| **Rare words** | Low values | Upweighted |\n",
    "| **Normalization** | Optional | Built-in (L2) |\n",
    "| **Best for** | Classification | Retrieval/Similarity |\n",
    "| **Complexity** | Simple | Moderate |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Preprocessing**\n",
    "   - Remove stop words (\"the\", \"is\", \"and\")\n",
    "   - Lowercase text\n",
    "   - Remove punctuation\n",
    "   - Consider lemmatization/stemming\n",
    "\n",
    "2. **Feature Selection**\n",
    "   - Use `max_features` to limit vocabulary size\n",
    "   - Use `min_df` and `max_df` to filter rare/common words\n",
    "   - Consider n-grams for capturing phrases\n",
    "\n",
    "3. **Normalization**\n",
    "   - L2 normalization for TF-IDF (default in sklearn)\n",
    "   - Consider binary BoW for presence/absence\n",
    "\n",
    "4. **Model Selection**\n",
    "   - Try both BoW and TF-IDF\n",
    "   - TF-IDF usually performs better\n",
    "   - For modern applications, consider word embeddings (Word2Vec, GloVe, BERT)\n",
    "\n",
    "### Moving Beyond Classical Methods\n",
    "\n",
    "While BoW and TF-IDF are useful, modern NLP often uses:\n",
    "- **Word Embeddings**: Word2Vec, GloVe, FastText\n",
    "- **Contextualized Embeddings**: BERT, GPT, RoBERTa\n",
    "- **Sentence Embeddings**: Sentence-BERT, Universal Sentence Encoder\n",
    "\n",
    "These methods capture semantic meaning and context better than classical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "1. **Preprocessing**: Implement a text preprocessor with stemming/lemmatization and compare results\n",
    "\n",
    "2. **N-gram Analysis**: Experiment with different n-gram ranges and analyze their impact on classification\n",
    "\n",
    "3. **Feature Engineering**: Create custom features combining BoW, TF-IDF, and linguistic features\n",
    "\n",
    "4. **Search Engine**: Extend the search engine with query expansion and relevance feedback\n",
    "\n",
    "5. **Document Clustering**: Use TF-IDF vectors with K-means clustering to group similar documents\n",
    "\n",
    "6. **Real Dataset**: Apply these techniques to a real dataset (e.g., 20 Newsgroups, IMDB reviews)\n",
    "\n",
    "7. **Comparison**: Compare classical methods with modern embeddings on the same task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
