{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-processing Techniques\n",
    "\n",
    "This notebook demonstrates essential text pre-processing techniques used in NLP pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Text\n",
    "\n",
    "Let's use a sample text to demonstrate all preprocessing techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Natural Language Processing (NLP) is AMAZING! It enables computers to understand human language. \n",
    "The students are studying various NLP techniques including tokenization, stemming, and lemmatization.\n",
    "These preprocessing steps are crucial for building effective NLP applications.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization\n",
    "\n",
    "Tokenization is the process of breaking text into smaller units (tokens) such as words or sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(sample_text)\n",
    "print(\"Sentence Tokenization:\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sent.strip()}\")\n",
    "\n",
    "print(f\"\\nTotal sentences: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Tokenization\n",
    "tokens = word_tokenize(sample_text)\n",
    "print(\"Word Tokenization:\")\n",
    "print(tokens)\n",
    "print(f\"\\nTotal tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normalization\n",
    "\n",
    "Normalization includes converting text to lowercase, removing punctuation, and standardizing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase\n",
    "text_lower = sample_text.lower()\n",
    "print(\"Lowercase Text:\")\n",
    "print(text_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "tokens = word_tokenize(text_lower)\n",
    "tokens_no_punct = [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "print(\"Tokens without punctuation:\")\n",
    "print(tokens_no_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stop-word Removal\n",
    "\n",
    "Stop words are common words (like 'the', 'is', 'are') that don't carry much meaning and can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"Sample stop words:\")\n",
    "print(list(stop_words)[:20])\n",
    "print(f\"\\nTotal stop words: {len(stop_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "tokens_no_stopwords = [word for word in tokens_no_punct if word not in stop_words]\n",
    "\n",
    "print(\"Before removing stop words:\")\n",
    "print(tokens_no_punct)\n",
    "print(f\"\\nAfter removing stop words:\")\n",
    "print(tokens_no_stopwords)\n",
    "print(f\"\\nTokens reduced from {len(tokens_no_punct)} to {len(tokens_no_stopwords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stemming\n",
    "\n",
    "Stemming reduces words to their root form by removing suffixes (may not always be a valid word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens_no_stopwords]\n",
    "\n",
    "print(\"Original vs Stemmed words:\")\n",
    "for original, stemmed in zip(tokens_no_stopwords, stemmed_words):\n",
    "    if original != stemmed:\n",
    "        print(f\"{original:20} -> {stemmed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming examples\n",
    "words_to_stem = ['running', 'runs', 'ran', 'runner', 'easily', 'fairly']\n",
    "\n",
    "print(\"Stemming Examples:\")\n",
    "for word in words_to_stem:\n",
    "    print(f\"{word:15} -> {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lemmatization\n",
    "\n",
    "Lemmatization reduces words to their base form (lemma) using vocabulary and morphological analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens_no_stopwords]\n",
    "\n",
    "print(\"Original vs Lemmatized words:\")\n",
    "for original, lemmatized in zip(tokens_no_stopwords, lemmatized_words):\n",
    "    if original != lemmatized:\n",
    "        print(f\"{original:20} -> {lemmatized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization with POS tags (more accurate)\n",
    "words_to_lemmatize = ['running', 'runs', 'ran', 'better', 'cacti', 'geese']\n",
    "\n",
    "print(\"Lemmatization Examples:\")\n",
    "for word in words_to_lemmatize:\n",
    "    # As verb\n",
    "    lemma_v = lemmatizer.lemmatize(word, pos='v')\n",
    "    # As noun\n",
    "    lemma_n = lemmatizer.lemmatize(word, pos='n')\n",
    "    print(f\"{word:15} -> verb: {lemma_v:12} noun: {lemma_n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stemming vs Lemmatization\n",
    "\n",
    "Let's compare both techniques side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare stemming and lemmatization\n",
    "test_words = ['studies', 'studying', 'studied', 'better', 'running', 'feet']\n",
    "\n",
    "print(f\"{'Word':<15} {'Stemmed':<15} {'Lemmatized':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for word in test_words:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    lemmatized = lemmatizer.lemmatize(word, pos='v')\n",
    "    print(f\"{word:<15} {stemmed:<15} {lemmatized:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Preprocessing Pipeline\n",
    "\n",
    "Let's combine all steps into a single preprocessing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, use_stemming=False, use_lemmatization=True):\n",
    "    \"\"\"\n",
    "    Complete text preprocessing pipeline\n",
    "    \"\"\"\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 3. Remove punctuation\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    \n",
    "    # 4. Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 5. Stemming or Lemmatization\n",
    "    if use_stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    elif use_lemmatization:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Test the pipeline\n",
    "print(\"Original text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nPreprocessed tokens:\")\n",
    "print(preprocess_text(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practice Exercise\n",
    "\n",
    "Try preprocessing your own text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! Try with your own text\n",
    "your_text = \"The children are playing in the beautiful garden. They were running and laughing happily!\"\n",
    "\n",
    "print(\"Your text:\")\n",
    "print(your_text)\n",
    "print(\"\\nPreprocessed (with lemmatization):\")\n",
    "print(preprocess_text(your_text, use_lemmatization=True))\n",
    "print(\"\\nPreprocessed (with stemming):\")\n",
    "print(preprocess_text(your_text, use_stemming=True, use_lemmatization=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
